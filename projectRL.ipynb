{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for trading\n",
    "This is a project to be able to predict what the best course of action is. Initially, I will only include some basic stats, such as the daily price of the dollar in Argentine pesos: both the official and the unofficial (blue). \n",
    "\n",
    "\n",
    "## Reminder\n",
    "\n",
    "We are going to use *Deep Q learning* \n",
    "\n",
    "There is an \"agent\" and an environment. The agent will be essentially a neural network (LSTM), that makes a decision. Initially, our decision will be: buy $100 USD, sell -te equivalent in pesos of- $100 USD, or hold . Then, the environment will inform if the decision was correct or not. The environment is nothing but the data that we give the agent (the price of the dollar in pesos, etc.)\n",
    "\n",
    "\n",
    "\n",
    "### Key distinctions\n",
    "Rewars is an immediate signal that is received in a given state, while value is the sum of all rewards you might anticipate from that state. Value is a long-term expectation, while reward is an immediate pleasure. \n",
    "You can have states where value and reward diverge.  \n",
    "\n",
    "### Objective function\n",
    "$$\n",
    "\\sum_{t=0}^{\\infty} \\gamma^t r(x(t), a(t))\n",
    "$$\n",
    "$x$ is the state at a given time step, and $a$ is the action taken in that state. $r$ is the reward. \n",
    "\n",
    "We are trying to maximize the sum of $r$ along, let's say, infinite time steps or whatever...\n",
    "\n",
    "$$\n",
    "Q(s, a) = r(s, a) + \\gamma \\max_{a} Q(s', a)\n",
    "$$\n",
    "\n",
    "This is another way to look at the objective function. Q function is recursive: for each step we calculate the immediate reward, then we get the max final reward. \n",
    "\n",
    "$\\gamma\\$ makes the immediate rewards more important. \n",
    "\n",
    "$$\n",
    "Q(s, a) \\rightarrow \\gamma Q(s', a) + \\gamma^2 Q(s'', a) + \\dots + \\gamma^n Q(s^{''\\dots n}, a)\n",
    "$$\n",
    "This is another way to look at this. It is essentially an expansion of the aboce recursive funciton. \n",
    "\n",
    "### Q-learning and Deep Q-learning\n",
    "\n",
    "Q-learning does not involve neural networks. Initially, it just assumes we can calculate every possible decision, and every possible state. \n",
    "\n",
    "This is where deep learning comes in. Essentially, instead of *calculating* Q function, we *estimate* the Q-function through a neural network. \n",
    "\n",
    "#### Loss function in Deep-Q learning\n",
    "\n",
    "The loss function here is mean squared error of the predicted Q-value and the target Q-value -Q*. This is basically a regression problem. \n",
    "\n",
    "\n",
    "## Custom reward function\n",
    "I want to apply a reward function that punishes losses exponentially, but rewards wins linearly. This is to make the system more conservative when \"gambling\" money. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "#...\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tensorflow.summary.create_file_writer(\"/tmp/tfvgg\")\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input_shape must be : Number of samples, number of time steps, and number of features.\n",
    "# input_shape = 30, 4 : would be 30 times-steps (about a month, 4 features)\n",
    "import time\n",
    "REPLAY_MEMORY_SIZE = 1500\n",
    "MIN_REPLAY_MEMORY_SIZE = 100\n",
    "MODEL_NAME = 'FIRST_MODEL'\n",
    "MINIBATCH_SIZE = 32\n",
    "DISCOUNT = 1 - (1/2**6)\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "TIME_STEP = 200\n",
    "FEATURES = 3\n",
    "INPUT_SHAPE = TIME_STEP, FEATURES \n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape_, layers, dropout):\n",
    "        # Main model\n",
    "        # gets trained every step\n",
    "        self.model = self.create_model(input_shape_, layers, dropout)\n",
    "\n",
    "        # Target network\n",
    "        # .predict every step\n",
    "        # every n steps, we update the model that we've been fitting for every step, and I guess we discard the old one ... \n",
    "        self.target_model = self.create_model(input_shape_, layers, dropout)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        # Custom tensorboard object\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "\n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0 \n",
    "        \n",
    "    def create_model(self, input_shape_, layers, dropout):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(layers[0],  input_shape=input_shape_))\n",
    "        \n",
    "        for i in range(1,len(layers)):\n",
    "            if dropout:\n",
    "                model.add(Dropout(dropout))\n",
    "            model.add(Dense(layers[i], activation=\"relu\"))\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "   \n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    \n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "                # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "        \n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.info is the price of the dollar in pesos\n",
    "CAUTION_FACTOR = 0.5 # multiplies the punishment for holding\n",
    "GAMBLER_PUNISHER = 1.6 # scales the punishment for buying or selling and loosing\n",
    "class BlobEnv:\n",
    "    def __init__(self, original_info):\n",
    "        self.original_info = original_info\n",
    "        self.info = original_info\n",
    "        self.total_episode_step = 0\n",
    "    def reset(self):\n",
    "        self.info = self.original_info[self.total_episode_step:]\n",
    "        self.episode_step = 0\n",
    "        self.negative_step = 0\n",
    "\n",
    "        return self.info\n",
    "    \n",
    "    # action will be one of three values. buy, sell, hold. \n",
    "    # I don't think we have the need for observation, which in the example is the state of the game. \n",
    "    # In this case, the state is simply given by the information we already have ... \n",
    "    # So, I'm not sure what to do with observation, really ... \n",
    "    # So, state is what we actually feed the model, so state is nothing other than the array we have, except that it's moved\n",
    "    # one bit over every time ...\n",
    "    # This will be a problem, because the size of the array will change, decisions ... decision ... \n",
    "     \n",
    "    def step(self, action):\n",
    "        print('hello')\n",
    "        self.episode_step += 1\n",
    "        self.total_episode_step += 1\n",
    "        diff = self.info[TIME_STEP][0] - self.info[TIME_STEP - 1][0]\n",
    "        print(f\"self.info[TIME_STEP] is {self.info[TIME_STEP][0]}\")\n",
    "        print(f\"diff is {diff}\")\n",
    "\n",
    "        if action == 0: # hold \n",
    "            # I think this is fine, it's the immediate reward\n",
    "            # I am getting\n",
    "            # Holding will always give you some punishment, because unless the price is \n",
    "            # exactly the same, it means you could have benefitted from selling or buying \n",
    "            # But, we don't want to encourage the system to be wild, so we will multiply\n",
    "            # this punishment by a caution factor, so the punishment will be less \n",
    "            # than if the system actually LOST the money\n",
    "            reward = - abs(diff) * CAUTION_FACTOR\n",
    "            self.negative_step += 1\n",
    "        elif action == 1: # buy dollars\n",
    "            if diff >= 0:\n",
    "                reward = diff/self.info[TIME_STEP][0]\n",
    "            else:\n",
    "                reward = -(diff ** GAMBLER_PUNISHER)/self.info[TIME_STEP][0]\n",
    "                self.negative_step += 1\n",
    "                \n",
    "        else: # sell dollars\n",
    "            if diff >= 0:\n",
    "                reward = -(diff ** GAMBLER_PUNISHER)/self.info[TIME_STEP][0]\n",
    "                self.negative_step += 1\n",
    "            else:\n",
    "                reward = diff/self.info[TIME_STEP][0]\n",
    "        self.info = self.original_info[self.total_episode_step:]\n",
    "        done = False\n",
    "        \n",
    "        # If you've accumulated 200 days with losses, time to stop ... \n",
    "        if self.episode_step  >= 300 or self.negative_step >= 150:\n",
    "            done = True\n",
    "        \n",
    "        return self.info, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dolar_blue = pd.read_csv(\"./Dolar_blue.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Compra_bl</th>\n",
       "      <th>Venta_bl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/2024</td>\n",
       "      <td>1185</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19/01/2024</td>\n",
       "      <td>1170</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18/01/2024</td>\n",
       "      <td>1190</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/01/2024</td>\n",
       "      <td>1175</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/01/2024</td>\n",
       "      <td>1130</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>16/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>15/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>14/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>13/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>12/01/2004</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4928 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fecha Compra_bl Venta_bl\n",
       "0     22/01/2024      1185     1235\n",
       "1     19/01/2024      1170     1220\n",
       "2     18/01/2024      1190     1240\n",
       "3     17/01/2024      1175     1225\n",
       "4     16/01/2024      1130     1180\n",
       "...          ...       ...      ...\n",
       "4923  16/01/2004       2,9     2,91\n",
       "4924  15/01/2004      2,88     2,89\n",
       "4925  14/01/2004       2,9     2,91\n",
       "4926  13/01/2004      2,88     2,89\n",
       "4927  12/01/2004      2,87     2,88\n",
       "\n",
       "[4928 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolar_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_blue = dolar_blue.drop_duplicates(subset=\"Fecha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Compra_bl</th>\n",
       "      <th>Venta_bl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/2024</td>\n",
       "      <td>1185</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19/01/2024</td>\n",
       "      <td>1170</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18/01/2024</td>\n",
       "      <td>1190</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/01/2024</td>\n",
       "      <td>1175</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/01/2024</td>\n",
       "      <td>1130</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>16/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>15/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>14/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>13/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>12/01/2004</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4928 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fecha Compra_bl Venta_bl\n",
       "0     22/01/2024      1185     1235\n",
       "1     19/01/2024      1170     1220\n",
       "2     18/01/2024      1190     1240\n",
       "3     17/01/2024      1175     1225\n",
       "4     16/01/2024      1130     1180\n",
       "...          ...       ...      ...\n",
       "4923  16/01/2004       2,9     2,91\n",
       "4924  15/01/2004      2,88     2,89\n",
       "4925  14/01/2004       2,9     2,91\n",
       "4926  13/01/2004      2,88     2,89\n",
       "4927  12/01/2004      2,87     2,88\n",
       "\n",
       "[4928 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolar_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_oficial = pd.read_csv(\"./Dolar_Oficial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_oficial = dolar_oficial.drop_duplicates(subset=\"Fecha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_oficial = dolar_oficial.rename(columns={\"Compra\":\"Compra_of\",\"Venta\":\"Venta_of\"})\n",
    "dolar_blue = dolar_blue.rename(columns={\"Compra\":\"Compra_bl\",\"Venta\":\"Venta_bl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Compra_bl</th>\n",
       "      <th>Venta_bl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/2024</td>\n",
       "      <td>1185</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19/01/2024</td>\n",
       "      <td>1170</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18/01/2024</td>\n",
       "      <td>1190</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/01/2024</td>\n",
       "      <td>1175</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/01/2024</td>\n",
       "      <td>1130</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>16/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>15/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>14/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>13/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>12/01/2004</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4928 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fecha Compra_bl Venta_bl\n",
       "0     22/01/2024      1185     1235\n",
       "1     19/01/2024      1170     1220\n",
       "2     18/01/2024      1190     1240\n",
       "3     17/01/2024      1175     1225\n",
       "4     16/01/2024      1130     1180\n",
       "...          ...       ...      ...\n",
       "4923  16/01/2004       2,9     2,91\n",
       "4924  15/01/2004      2,88     2,89\n",
       "4925  14/01/2004       2,9     2,91\n",
       "4926  13/01/2004      2,88     2,89\n",
       "4927  12/01/2004      2,87     2,88\n",
       "\n",
       "[4928 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolar_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar_oficial.to_csv(\"Dolar_Oficial.csv\", index=False)\n",
    "dolar_blue.to_csv(\"Dolar_blue.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Compra_bl</th>\n",
       "      <th>Venta_bl</th>\n",
       "      <th>Compra_of</th>\n",
       "      <th>Venta_of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/2024</td>\n",
       "      <td>1185</td>\n",
       "      <td>1235</td>\n",
       "      <td>808,08</td>\n",
       "      <td>868,8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19/01/2024</td>\n",
       "      <td>1170</td>\n",
       "      <td>1220</td>\n",
       "      <td>808,08</td>\n",
       "      <td>868,8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18/01/2024</td>\n",
       "      <td>1190</td>\n",
       "      <td>1240</td>\n",
       "      <td>807,2</td>\n",
       "      <td>866,51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/01/2024</td>\n",
       "      <td>1175</td>\n",
       "      <td>1225</td>\n",
       "      <td>806,96</td>\n",
       "      <td>866,2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/01/2024</td>\n",
       "      <td>1130</td>\n",
       "      <td>1180</td>\n",
       "      <td>805,38</td>\n",
       "      <td>864,65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>19/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>16/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>15/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "      <td>2,86</td>\n",
       "      <td>2,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>14/01/2004</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "      <td>2,86</td>\n",
       "      <td>2,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>13/01/2004</td>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "      <td>2,85</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3095 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fecha Compra_bl Venta_bl Compra_of Venta_of\n",
       "0     22/01/2024      1185     1235    808,08    868,8\n",
       "1     19/01/2024      1170     1220    808,08    868,8\n",
       "2     18/01/2024      1190     1240     807,2   866,51\n",
       "3     17/01/2024      1175     1225    806,96    866,2\n",
       "4     16/01/2024      1130     1180    805,38   864,65\n",
       "...          ...       ...      ...       ...      ...\n",
       "3090  19/01/2004       2,9      2,9      2,87     2,91\n",
       "3091  16/01/2004       2,9     2,91      2,87     2,91\n",
       "3092  15/01/2004      2,88     2,89      2,86      2,9\n",
       "3093  14/01/2004       2,9     2,91      2,86      2,9\n",
       "3094  13/01/2004      2,88     2,89      2,85     2,89\n",
       "\n",
       "[3095 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dollars = dolar_blue.merge(right=dolar_oficial, how=\"inner\")\n",
    "all_dollars.to_csv(\"./dolar_todos.csv\",index=False)\n",
    "all_dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_all_dollars = all_dollars.iloc[:,1:].apply(lambda x:x.str.replace(',','.').astype(float),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compra_bl</th>\n",
       "      <th>Venta_bl</th>\n",
       "      <th>Compra_of</th>\n",
       "      <th>Venta_of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185</td>\n",
       "      <td>1235</td>\n",
       "      <td>808,08</td>\n",
       "      <td>868,8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1170</td>\n",
       "      <td>1220</td>\n",
       "      <td>808,08</td>\n",
       "      <td>868,8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1190</td>\n",
       "      <td>1240</td>\n",
       "      <td>807,2</td>\n",
       "      <td>866,51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1225</td>\n",
       "      <td>806,96</td>\n",
       "      <td>866,2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1130</td>\n",
       "      <td>1180</td>\n",
       "      <td>805,38</td>\n",
       "      <td>864,65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>2,9</td>\n",
       "      <td>2,9</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "      <td>2,87</td>\n",
       "      <td>2,91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "      <td>2,86</td>\n",
       "      <td>2,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>2,9</td>\n",
       "      <td>2,91</td>\n",
       "      <td>2,86</td>\n",
       "      <td>2,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>2,88</td>\n",
       "      <td>2,89</td>\n",
       "      <td>2,85</td>\n",
       "      <td>2,89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3095 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Compra_bl Venta_bl Compra_of Venta_of\n",
       "0         1185     1235    808,08    868,8\n",
       "1         1170     1220    808,08    868,8\n",
       "2         1190     1240     807,2   866,51\n",
       "3         1175     1225    806,96    866,2\n",
       "4         1130     1180    805,38   864,65\n",
       "...        ...      ...       ...      ...\n",
       "3090       2,9      2,9      2,87     2,91\n",
       "3091       2,9     2,91      2,87     2,91\n",
       "3092      2,88     2,89      2,86      2,9\n",
       "3093       2,9     2,91      2,86      2,9\n",
       "3094      2,88     2,89      2,85     2,89\n",
       "\n",
       "[3095 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dollars.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1235.  ,  808.08,  868.8 ],\n",
       "       [1220.  ,  808.08,  868.8 ],\n",
       "       [1240.  ,  807.2 ,  866.51],\n",
       "       ...,\n",
       "       [   2.89,    2.86,    2.9 ],\n",
       "       [   2.91,    2.86,    2.9 ],\n",
       "       [   2.89,    2.85,    2.89]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dollars = np.array(only_all_dollars.iloc[:,1:])\n",
    "dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = DQNAgent(INPUT_SHAPE,[32,32,23],.2)\n",
    "env = BlobEnv(dollars)\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(23)\n",
    "np.random.seed(23)\n",
    "tensorflow.random.set_seed(23)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "self.info[TIME_STEP] is [288.  145.9 153.9]\n",
      "[ 0.   -0.35 -0.35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Get random action  ????? \u001b[39;00m\n\u001b[1;32m     34\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, ACTION_SPACE_SIZE)\n\u001b[0;32m---> 36\u001b[0m new_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Transform new continuous state to a new discrete state and count reward \u001b[39;00m\n\u001b[1;32m     39\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[87], line 50\u001b[0m, in \u001b[0;36mBlobEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# sell dollars\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m         reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(diff \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m GAMBLER_PUNISHER)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[TIME_STEP] \n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "EPISODES = 4000\n",
    "# Exploration settings\n",
    "epsilon = 0.6  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "ACTION_SPACE_SIZE = 3 # number of possible decisions ... \n",
    "AGGREGATE_STATS_EVERY = 50\n",
    "ep_rewards = [0]\n",
    "MIN_REWARD = -1\n",
    "# Iterate over episodes\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "    # Update tensorboard step every episode\n",
    "    \n",
    "    agent.tensorboard.step = episode # ????\n",
    "    \n",
    "    # Restarting episode = reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    \n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        \n",
    "        else:\n",
    "            # Get random action  ????? \n",
    "            action = np.random.randint(0, ACTION_SPACE_SIZE)\n",
    "            \n",
    "        new_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Transform new continuous state to a new discrete state and count reward \n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        \n",
    "        current_state = new_state \n",
    "        step += 1\n",
    "        \n",
    "        # Append episode reward to a list and log stats (every given number of episodes)\n",
    "        ep_rewards.append(episode_reward)  \n",
    "        print(f\"ep_rewards is {ep_rewards}\")\n",
    "        \n",
    "        if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "            print(f\"ep_rewards is {ep_rewards}\")\n",
    "            average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "            \n",
    "            # Save model, but only when min reward is greater or equal a set value\n",
    "            if min_reward >= MIN_REWARD:\n",
    "               agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > MIN_EPSILON:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "            epsilon = max(MIN_EPSILON, epsilon)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for trading\n",
    "This is a project to be able to predict what the best course of action is. Initially, I will only include some basic stats, such as the daily price of the dollar in Argentine pesos: both the official and the unofficial (blue). \n",
    "\n",
    "\n",
    "## Reminder\n",
    "\n",
    "We are going to use *Deep Q learning* \n",
    "\n",
    "There is an \"agent\" and an environment. The agent will be essentially a neural network (LSTM), that makes a decision. Initially, our decision will be: buy $100 USD, sell -te equivalent in pesos of- $100 USD, or hold . Then, the environment will inform if the decision was correct or not. The environment is nothing but the data that we give the agent (the price of the dollar in pesos, etc.)\n",
    "\n",
    "\n",
    "\n",
    "### Key distinctions\n",
    "Rewars is an immediate signal that is received in a given state, while value is the sum of all rewards you might anticipate from that state. Value is a long-term expectation, while reward is an immediate pleasure. \n",
    "You can have states where value and reward diverge.  \n",
    "\n",
    "### Objective function\n",
    "$$\n",
    "\\sum_{t=0}^{\\infty} \\gamma^t r(x(t), a(t))\n",
    "$$\n",
    "$x$ is the state at a given time step, and $a$ is the action taken in that state. $r$ is the reward. \n",
    "\n",
    "We are trying to maximize the sum of $r$ along, let's say, infinite time steps or whatever...\n",
    "\n",
    "$$\n",
    "Q(s, a) = r(s, a) + \\gamma \\max_{a} Q(s', a)\n",
    "$$\n",
    "\n",
    "This is another way to look at the objective function. Q function is recursive: for each step we calculate the immediate reward, then we get the max final reward. \n",
    "\n",
    "$\\gamma\\$ makes the immediate rewards more important. \n",
    "\n",
    "$$\n",
    "Q(s, a) \\rightarrow \\gamma Q(s', a) + \\gamma^2 Q(s'', a) + \\dots + \\gamma^n Q(s^{''\\dots n}, a)\n",
    "$$\n",
    "This is another way to look at this. It is essentially an expansion of the aboce recursive funciton. \n",
    "\n",
    "### Q-learning and Deep Q-learning\n",
    "\n",
    "Q-learning does not involve neural networks. Initially, it just assumes we can calculate every possible decision, and every possible state. \n",
    "\n",
    "This is where deep learning comes in. Essentially, instead of *calculating* Q function, we *estimate* the Q-function through a neural network. \n",
    "\n",
    "#### Loss function in Deep-Q learning\n",
    "\n",
    "The loss function here is mean squared error of the predicted Q-value and the target Q-value -Q*. This is basically a regression problem. \n",
    "\n",
    "\n",
    "## Custom reward function\n",
    "I want to apply a reward function that punishes losses exponentially, but rewards wins linearly. This is to make the system more conservative when \"gambling\" money. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
